Tuyá»‡t vá»i Linh! DÆ°á»›i Ä‘Ã¢y lÃ  hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c Ä‘á»ƒ báº¡n lÃ m quen vá»›i **AWS Glue** vÃ  sá»­ dá»¥ng nÃ³ Ä‘á»ƒ thá»±c hiá»‡n ETL dá»¯ liá»‡u tá»« file CSV trong S3 sang DynamoDB. HÆ°á»›ng dáº«n nÃ y sá»­ dá»¥ng **AWS Console**, khÃ´ng cáº§n viáº¿t code thá»§ cÃ´ng.

---

## ğŸ§­ HÆ°á»›ng dáº«n tá»«ng bÆ°á»›c: ETL tá»« S3 (CSV) sang DynamoDB báº±ng AWS Glue

### **BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  tÃ i nguyÃªn**

- âœ… **File CSV**: Äáº£m báº£o file cÃ³ header dÃ²ng Ä‘áº§u tiÃªn, vÃ­ dá»¥:
  ```csv
  id,name,email
  1,Alice,alice@example.com
  2,Bob,bob@example.com
  ```
  LÆ°u vÃ o S3, vÃ­ dá»¥: `s3://my-etl-bucket/users.csv`

- âœ… **DynamoDB Table**: Táº¡o báº£ng DynamoDB trÆ°á»›c, vÃ­ dá»¥:
  - TÃªn báº£ng: `users`
  - Partition key: `id` (kiá»ƒu Number)

---

### **BÆ°á»›c 2: Táº¡o Glue Database**

1. VÃ o **AWS Glue Console** â†’ **Databases** â†’ **Add database**
2. Nháº­p tÃªn: `etl_demo_db`

---

### **BÆ°á»›c 3: Táº¡o Glue Crawler Ä‘á»ƒ khÃ¡m phÃ¡ dá»¯ liá»‡u CSV**

1. VÃ o **Crawlers** â†’ **Add crawler**
2. Äáº·t tÃªn: `s3_csv_crawler`
3. Chá»n nguá»“n dá»¯ liá»‡u:
   - Data store: S3
   - Path: `s3://my-etl-bucket/users.csv`
4. Chá»n IAM role: `AWSGlueServiceRoleDefault` hoáº·c táº¡o role má»›i cÃ³ quyá»n truy cáº­p S3 vÃ  DynamoDB.
5. Output:
   - Database: `etl_demo_db`
   - Table prefix: `csv_`
6. Cháº¡y crawler â†’ kiá»ƒm tra báº£ng Ä‘Æ°á»£c táº¡o, vÃ­ dá»¥: `csv_users`

---

### **BÆ°á»›c 4: Táº¡o Glue Job Ä‘á»ƒ ETL sang DynamoDB**

1. VÃ o **Jobs** â†’ **Add job**
2. TÃªn: `csv_to_dynamodb_job`
3. IAM Role: chá»n role cÃ³ quyá»n `glue`, `s3:GetObject`, `dynamodb:PutItem`
4. Type: **Spark**, Language: **Python**
5. Source: chá»n báº£ng `csv_users` tá»« Glue Catalog
6. Target: chá»n **DynamoDB** â†’ báº£ng `users`
7. Mapping: Ä‘áº£m báº£o cÃ¡c cá»™t `id`, `name`, `email` Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘Ãºng
8. Script: Glue sáº½ tá»± táº¡o mÃ£ ETL báº±ng PySpark. Báº¡n cÃ³ thá»ƒ chá»‰nh sá»­a náº¿u cáº§n.

VÃ­ dá»¥ Ä‘oáº¡n mÃ£ PySpark:

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Äá»c tá»« Glue Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(database = "etl_demo_db", table_name = "csv_users")

# Ghi vÃ o DynamoDB
glueContext.write_dynamic_frame.from_options(
    frame = datasource,
    connection_type = "dynamodb",
    connection_options = {"dynamodb.output.tableName": "users", "dynamodb.throughput.write.percent": "1.0"}
)

job.commit()
```

---

### **BÆ°á»›c 5: Cháº¡y Job vÃ  kiá»ƒm tra káº¿t quáº£**

- Cháº¡y job tá»« Glue Console.
- VÃ o DynamoDB â†’ kiá»ƒm tra báº£ng `users` â†’ dá»¯ liá»‡u tá»« CSV Ä‘Ã£ Ä‘Æ°á»£c insert.

---

